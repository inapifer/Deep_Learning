{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing numpy as the main library\n",
    "import numpy as np\n",
    "# I import main_functions which will have functions such as activation functions and others\n",
    "from main_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing weights randomly by neurons per layer and the number of layers\n",
    "def init_weights(number_layers, neurons_by_layer):\n",
    "    #Initializing weights and biases.\n",
    "    weights = {}\n",
    "    biases = {}\n",
    "    \n",
    "    #Doing a for loop across all the layers.\n",
    "    for l in range(1, number_layers):\n",
    "        #Returning random values of weights from the layer number l\n",
    "        weights[\"W\" + str(l)] = np.random.rand(neurons_by_layer[l], neurons_by_layer[l-1])\n",
    "        biases[\"b\" + str(l)] = np.random.rand(neurons_by_layer[l], 1)\n",
    "    \n",
    "    return weights, biases    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing a function that performs a whole forward iteration for the Neural Network\n",
    "def forward_iteration(X, weights, biases, activation_names, number_layers, neurons_by_layer):\n",
    "    \"\"\"This function receives the inputs X (A[0]), the weights from the whole network by a dictionary,\n",
    "    the biases by a dictionary and the activation names in a list from each layer. It keeps track \n",
    "    of the linear activations and the activations of the whole network in order to use it in the backpropagation\n",
    "     algorithm and finally it gives the final result of the network\"\"\"\n",
    "    \n",
    "    # I begin the dictionary of the activated neurons and save A[0] by the input of the network.\n",
    "    A_cache = {\"A0\": X}\n",
    "    Z_cache = {}\n",
    "    \n",
    "    for l in range(1, number_layers):\n",
    "        #Because the activation names start from the layer 1 I have to call as activation_name of layer 1 =\n",
    "        # activation_names[0], activation_names of layer 2 = activation_names[1] ...\n",
    "        activation_name = activation_names[l-1]\n",
    "        \n",
    "        #The weights and  biases defined by the dictionaries \n",
    "        A_prev = A_cache[\"A\" + str(l-1)]\n",
    "        W = weights[\"W\" + str(l)]\n",
    "        b = biases[\"b\" + str(l)]\n",
    "        \n",
    "        #Doing the linear activation and the forward activation in the layer l. It's worth to notice that I need\n",
    "        # to get Z because I will need those values for the backpropagation algorithm.\n",
    "        Z = linear_activation(W, A_prev, b)\n",
    "        A = forward_activation(W, A_prev, b, activation_name)\n",
    "        # Now I can save those values in the caches of A and Z\n",
    "        A_cache[\"A\" + str(l)] = A\n",
    "        Z_cache[\"Z\" +str(l)] = Z\n",
    "        \n",
    "  # Finally, I save the final value of the network Y, which is the value of the activation function in the last layer.    \n",
    "        \n",
    "    Y_hat = A_cache[\"A\" + str(number_layers-1)]\n",
    "        \n",
    "    return Z_cache, A_cache, Y_hat    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: You still need to do the loss function and calculate the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_iteration(dA_L, Z_cache, A_cache, activation_names, number_layers, neurons_by_layer):\n",
    "    # Note: I don't still know if I need all these parameters.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I assume that I am going to finish the network with the sigmoid function, I will use the next loss function:\n",
    "\n",
    "$  L(\\hat{Y}, Y) = -(y\\log(\\hat{y}) + (1-y)\\log(1-\\hat{y}))$\n",
    "\n",
    "$ Cost = J(w, b) = \\frac{1}{m}\\sum_{i=1}^{m} L(\\hat{y}^{(i)}, y^{(i)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ da^{[L]} = \\frac{-y}{\\hat{y}} + \\frac{(1-y)}{(1-\\hat{y})} $$\n",
    "$$ \\text{Being } \\hat{y} \\equiv \\text{Activation from the last layer } a^{[L]}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I start the backpropagation algorithm.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10) \n",
      "\n",
      " [[ 1.02086136e+00  1.00000000e+00  1.02086136e+00  1.00000000e+00\n",
      "   1.00000000e+00 -6.11867023e+01 -4.57043849e+12 -7.02075641e+03\n",
      "  -4.89355116e+01  1.00017900e+00]]\n",
      "(1, 10)\n",
      "(3, 10)\n",
      "[[0.91391548 0.41973546 0.54019152]]\n",
      "dZ: (1, 10)\n",
      "db: (1, 1)\n",
      "dW: (1, 3)\n",
      "dA_prev: (3, 10)\n"
     ]
    }
   ],
   "source": [
    "#Draft where I implement ideas\n",
    "# I use the seed to maintain the random numbers fixed\n",
    "np.random.seed(2)\n",
    "X = np.random.randn(5,10)*10\n",
    "neurons_by_layer = [5, 4, 4, 3, 1]\n",
    "number_layers = len(neurons_by_layer)\n",
    "activation_names = ['relu', 'relu', 'relu', 'sigmoid']\n",
    "\n",
    "weights, biases = init_weights(number_layers, neurons_by_layer)\n",
    "\n",
    "Z_cache, A_cache, Y_hat = forward_iteration(X, weights, biases, activation_names, number_layers, neurons_by_layer)\n",
    "n_hat, m_hat = Y_hat.shape\n",
    "Y = np.random.randint(low=0, high=2, size=(n_hat, m_hat))\n",
    "\n",
    "dA_L = derivative_cost_logistic(Y_hat, Y)\n",
    "print(dA_L.shape, 2*'\\n', dA_L)\n",
    "\n",
    "Z_L = Z_cache['Z4']\n",
    "A_L_prev = A_cache['A3']\n",
    "activation_name = 'sigmoid'\n",
    "W = weights['W4']\n",
    "\n",
    "print(Z_L.shape)\n",
    "print(A_L_prev.shape)\n",
    "print(W)\n",
    "\n",
    "dZ, dW, db, dA_prev = backward_iteration(dA_L, Z_L, activation_name, A_L_prev, W)\n",
    "print(\"dZ: \" + str(dZ.shape))\n",
    "print(\"db: \" + str(db.shape))\n",
    "print(\"dW: \" + str(dW.shape))\n",
    "print(\"dA_prev: \" + str(dA_prev.shape))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['A0', 'A1', 'A2', 'A3', 'A4'])\n",
      "dict_keys(['Z1', 'Z2', 'Z3', 'Z4'])\n",
      "[[2.04350576e-02 8.28779304e-77 2.04350576e-02 1.18260130e-18\n",
      "  3.65854559e-11 1.63434204e-02 2.18797387e-13 1.42434795e-04\n",
      "  2.04350576e-02 1.78969889e-04]] \n",
      "\n",
      "\n",
      "[[2.04350576e-02 8.28779304e-77 2.04350576e-02 1.18260130e-18\n",
      "  3.65854559e-11 1.63434204e-02 2.18797387e-13 1.42434795e-04\n",
      "  2.04350576e-02 1.78969889e-04]]\n"
     ]
    }
   ],
   "source": [
    "print(A_cache.keys())\n",
    "print(Z_cache.keys())\n",
    "print(A_cache['A4'], 2*'\\n')\n",
    "print(Y_hat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
