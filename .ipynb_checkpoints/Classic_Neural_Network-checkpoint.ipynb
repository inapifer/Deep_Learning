{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing numpy as the main library\n",
    "import numpy as np\n",
    "# I import main_functions which will have functions such as activation functions and others\n",
    "from main_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing weights randomly by neurons per layer and the number of layers\n",
    "def init_weights(number_layers, neurons_by_layer):\n",
    "    #Initializing weights and biases.\n",
    "    weights = {}\n",
    "    biases = {}\n",
    "    \n",
    "    #Doing a for loop across all the layers.\n",
    "    for l in range(1, number_layers):\n",
    "        #Returning random values of weights from the layer number l\n",
    "        weights[\"W\" + str(l)] = np.random.rand(neurons_by_layer[l], neurons_by_layer[l-1])\n",
    "        biases[\"b\" + str(l)] = np.random.rand(neurons_by_layer[l], 1)\n",
    "    \n",
    "    return weights, biases    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing a function that performs a whole forward iteration for the Neural Network\n",
    "def forward_prop_iteration(X, weights, biases, activation_names, number_layers, neurons_by_layer):\n",
    "    \"\"\"This function receives the inputs X (A[0]), the weights from the whole network by a dictionary,\n",
    "    the biases by a dictionary and the activation names in a list from each layer. It keeps track \n",
    "    of the linear activations and the activations of the whole network in order to use it in the backpropagation\n",
    "     algorithm and finally it gives the final result of the network\"\"\"\n",
    "    \n",
    "    # I begin the dictionary of the activated neurons and save A[0] by the input of the network.\n",
    "    A_cache = {\"A0\": X}\n",
    "    Z_cache = {}\n",
    "    \n",
    "    for l in range(1, number_layers):\n",
    "        #Because the activation names start from the layer 1 I have to call as activation_name of layer 1 =\n",
    "        # activation_names[0], activation_names of layer 2 = activation_names[1] ...\n",
    "        activation_name = activation_names[l-1]\n",
    "        \n",
    "        #The weights and  biases defined by the dictionaries \n",
    "        A_prev = A_cache[\"A\" + str(l-1)]\n",
    "        W = weights[\"W\" + str(l)]\n",
    "        b = biases[\"b\" + str(l)]\n",
    "        \n",
    "        #Doing the linear activation and the forward activation in the layer l. It's worth to notice that I need\n",
    "        # to get Z because I will need those values for the backpropagation algorithm.\n",
    "        Z = linear_activation(W, A_prev, b)\n",
    "        A = forward_activation(W, A_prev, b, activation_name)\n",
    "        # Now I can save those values in the caches of A and Z\n",
    "        A_cache[\"A\" + str(l)] = A\n",
    "        Z_cache[\"Z\" +str(l)] = Z\n",
    "        \n",
    "  # Finally, I save the final value of the network Y, which is the value of the activation function in the last layer.    \n",
    "        \n",
    "    Y_hat = A_cache[\"A\" + str(number_layers-1)]\n",
    "        \n",
    "    return Z_cache, A_cache, Y_hat    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After having made the forward propagation iteration I calculate the cost of the network and the derivative of\n",
    "# the cost with respect to A_L\n",
    "\n",
    "def cost_function_and_first_derivative(Y_hat, Y, name_loss='binary_class'):\n",
    "    \"\"\"In this function I combine the function cost_function from the main_functions script\n",
    "    and I calculate the first derivative of the cost function with respect to A[L] in order to start\n",
    "    the backpropagation algorithm.\"\"\"\n",
    "    \n",
    "    cost = cost_function(Y_hat, Y, name_loss)\n",
    "    dA_L = derivative_cost_logistic(Y_hat, Y)\n",
    "    \n",
    "    # I return the cost of the network and the derivative dA_L(Derivative of the cost function with respect\n",
    "    # to A[L])\n",
    "    return cost, dA_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop_iteration(dA_L, Z_cache, A_cache, weights, biases, activation_names,\\\n",
    "                              number_layers, neurons_by_layer):\n",
    "    \n",
    "    \"\"\"A function that makes an iteration of the backpropagation algorithm through the whole network.\"\"\"\n",
    "    \n",
    "    # I assert that the number of layers minus 1 is equal to the length of the activation names and\n",
    "    # the length of neurons_by_layer is equal to number_layers\n",
    "    assert ((number_layers-1)==len(activation_names)) and (number_layers==len(neurons_by_layer)),\\\n",
    "    \"The number of layers must be equal to the length of neurons_by_layer and the length of activation_names\\\n",
    "    must be equal to the number of layers minus 1.\"\n",
    "    \n",
    "    # I create  dictionary dW_cache and db_cache which will save the derivatives of the weights and biases from\n",
    "    # the network.\n",
    "    dW_cache = {}\n",
    "    db_cache = {}\n",
    "    \n",
    "    \n",
    "    #I define the first dA as dA_L.\n",
    "    dA = dA_L\n",
    "    \n",
    "    # Here I start to iterate from the last layer (which index is [L-1] given that python starts in 0) \n",
    "    # to the layer 1 included. In python it takes the first value but not the last value of the loop,\n",
    "    # so I have iterate from [L-1] to 0\n",
    "    \n",
    "    for l in range(number_layers-1, 0, -1):\n",
    "        # As the activation_names list has an element less, we need to consider that activation_index= l-1\n",
    "        act_ix = l-1\n",
    "        # Now I take the activation name of this layer.\n",
    "        activation_name = activation_names[act_ix]\n",
    "        \n",
    "        # The linear activation Z[l] is taken from the Z_cache dictionary and the activation A[l-1] is taken\n",
    "        # from the A_cache dictionary. I also take the weights W of this layer from the dictionary weights.\n",
    "        Z = Z_cache['Z' + str(l)]\n",
    "        A_prev = A_cache['A' + str(l-1)]\n",
    "        W = weights['W' + str(l)]\n",
    "        \n",
    "        # Now I calculate dZ, dW, db and dA with the backward_iteration function from the main_functions script.\n",
    "        dZ, dW, db, dA_prev = backward_iteration(dA, Z, activation_name, A_prev, W)\n",
    "        \n",
    "        # Now I need to update dA to be the value of dA_prev in order to get the derivatives of the next \n",
    "        # iteration. It's not necessary to update dZ because the backward_iteration function does that internally.\n",
    "        dA = dA_prev\n",
    "        \n",
    "        \n",
    "        # I return now only the derivatives of the weights and biases because they are the parameters that are \n",
    "        # trained by the network.\n",
    "        dW_cache['dW' + str(l)] = dW\n",
    "        db_cache['db' + str(l)] = db\n",
    "        \n",
    "        \n",
    "    return dW_cache, db_cache    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I assume that I am going to finish the network with the sigmoid function, I will use the next loss function:\n",
    "\n",
    "$  L(\\hat{Y}, Y) = -(y\\log(\\hat{y}) + (1-y)\\log(1-\\hat{y}))$\n",
    "\n",
    "$ Cost = J(w, b) = \\frac{1}{m}\\sum_{i=1}^{m} L(\\hat{y}^{(i)}, y^{(i)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ da^{[L]} = \\frac{-y}{\\hat{y}} + \\frac{(1-y)}{(1-\\hat{y})} $$\n",
    "$$ \\text{Being } \\hat{y} \\equiv \\text{Activation from the last layer } a^{[L]}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(weights, biases, dW_cache, db_cache, number_layers, learning_rate=0.01):\n",
    "    \n",
    "    \"\"\"This function will return the weights and biases updated after having calculated the derivatives of\n",
    "    the parameters through the whole network and considering a learning rate. It's important to notice\n",
    "    that the operations are not done inplace.\"\"\"\n",
    "    \n",
    "    #First, let's assert that we are giving correctly the parameters.\n",
    "    assert (len(weights) and len(biases) and len(dW_cache) and len(db_cache)) == number_layers-1, \\\n",
    "    \"The length of weights, biases, dW_cache and db_cache must be equal to the number of layers -1\"\n",
    "    \n",
    "    weights_updated = {}\n",
    "    biases_updated = {}\n",
    "    \n",
    "    # I will go through the whole network from layer 1 to layer L-1 (Remember that the index starts with 0).\n",
    "    # The for loop stops at the index [number_layers - 1]\n",
    "    for l in range(1,number_layers):\n",
    "        weights_updated[\"W\" + str(l)] = weights[\"W\" + str(l)] - learning_rate*dW_cache[\"dW\" + str(l)]\n",
    "        biases_updated[\"b\" + str(l)] = biases[\"b\" + str(l)] - learning_rate*db_cache[\"db\" + str(l)]\n",
    "        \n",
    "    return weights_updated, biases_updated    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_network(X, Y, neurons_by_layer, number_layers, activation_names, name_loss='binary_class',\\\n",
    "                     learning_rate=0.01, number_iterations=50):\n",
    "    \n",
    "    \"\"\"A function that implements the whole training of the network on some number of iterations.\"\"\"\n",
    "    \n",
    "    # Although it's redundant giving the neurons by layer and the number of layers, on this way we can make sure that\n",
    "    # we are inserting the number of layers we truly want and its architecture.\n",
    "    \n",
    "    assert len(neurons_by_layer) == number_layers, \"The number of layers must be the same as the length of the \\\n",
    "    list of neurons_by_layer\"\n",
    "    \n",
    "    #Let's initialize randomly the weights and biases of the network.\n",
    "    weights, biases = init_weights(number_layers, neurons_by_layer)\n",
    "    \n",
    "    # Now we iterate over the network on the number of iterations.\n",
    "    \n",
    "    for iteration in range(number_iterations):\n",
    "        \n",
    "        # First we make the forward propagation were we get the caches and the final value of the network.\n",
    "        Z_cache, A_cache, Y_hat = forward_prop_iteration(X, weights, biases, activation_names,\\\n",
    "                                      number_layers, neurons_by_layer)\n",
    "        \n",
    "        # Now we get the cost of the network at this iteration and the derivative dA_L to make possible start\n",
    "        # the backpropagation algorithm. By default we use the loss function binary_class\n",
    "        cost, dA_L = cost_function_and_first_derivative(Y_hat, Y, name_loss)\n",
    "        \n",
    "        # If the iteration we are in is a multiple of 100 we show the cost of the network\n",
    "        if iteration%10==0:\n",
    "            print('Cost in iteration ' + str(iteration) + \": \" + str(cost))\n",
    "        \n",
    "        # After calculating the cost and the first derivative, we can start the backpropagation algorithm and\n",
    "        # get the derivatives of the weights and biases.\n",
    "        dW_cache, db_cache = back_prop_iteration(dA_L, Z_cache, A_cache, weights, biases, activation_names,\\\n",
    "                              number_layers, neurons_by_layer)\n",
    "        \n",
    "        #After getting the derivatives we can update the parameters of the network and go to the next iteration.\n",
    "        weights, biases = update_parameters(weights, biases, dW_cache, db_cache, number_layers,\\\n",
    "                                            learning_rate)\n",
    "        \n",
    "        \n",
    "        \n",
    "    # After going through all the iterations we return the parameters trained of the network and the cost    \n",
    "        \n",
    "        \n",
    "    return weights, biases, cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Draft where I implement ideas\n",
    "# I use the seed to maintain the random numbers fixed\n",
    "np.random.seed(2)\n",
    "X = np.random.randn(5,12)*10\n",
    "neurons_by_layer = [5, 4, 4, 3, 1]\n",
    "number_layers = len(neurons_by_layer)\n",
    "activation_names = ['relu', 'relu', 'relu', 'sigmoid']\n",
    "\n",
    "weights, biases = init_weights(number_layers, neurons_by_layer)\n",
    "\n",
    "Z_cache, A_cache, Y_hat = forward_prop_iteration(X, weights, biases, activation_names, number_layers, neurons_by_layer)\n",
    "n_hat, m_hat = Y_hat.shape\n",
    "Y = np.random.randint(low=0, high=2, size=(n_hat, m_hat))\n",
    "\n",
    "\n",
    "cost, dA_L = cost_function_and_first_derivative(Y_hat, Y, name_loss='binary_class')\n",
    "\n",
    "dW_cache, db_cache = back_prop_iteration(dA_L, Z_cache, A_cache, weights, biases,\\\n",
    "                                        activation_names, number_layers, neurons_by_layer)\n",
    "\n",
    "weights_updated, biases_updated = update_parameters(weights, biases, dW_cache, db_cache, number_layers, learning_rate=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "X = np.random.randn(5,12)*10\n",
    "n, m = X.shape\n",
    "\n",
    "neurons_by_layer = [5, 4, 4, 3, 1]\n",
    "number_layers = 5\n",
    "activation_names = ['relu', 'relu', 'relu', 'sigmoid']\n",
    "Y = np.random.randint(low=0, high=2, size=(1,m))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
