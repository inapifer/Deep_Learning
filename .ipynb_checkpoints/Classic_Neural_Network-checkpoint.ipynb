{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing numpy as the main library\n",
    "import numpy as np\n",
    "# I import main_functions which will have functions such as activation functions and others\n",
    "from main_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing weights randomly by neurons per layer and the number of layers\n",
    "def init_weights(number_layers, neurons_by_layer):\n",
    "    #Initializing weights and biases.\n",
    "    weights = {}\n",
    "    biases = {}\n",
    "    \n",
    "    #Doing a for loop across all the layers.\n",
    "    for l in range(1, number_layers):\n",
    "        #Returning random values of weights from the layer number l\n",
    "        weights[\"W\" + str(l)] = np.random.rand(neurons_by_layer[l], neurons_by_layer[l-1])\n",
    "        biases[\"b\" + str(l)] = np.random.rand(neurons_by_layer[l], 1)\n",
    "    \n",
    "    return weights, biases    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing a function that performs a whole forward iteration for the Neural Network\n",
    "def forward_iteration(X, weights, biases, activation_names, number_layers, neurons_by_layer):\n",
    "    \"\"\"This function receives the inputs X (A[0]), the weights from the whole network by a dictionary,\n",
    "    the biases by a dictionary,the activation names in a list from each layer.\"\"\"\n",
    "    \n",
    "    # I begin the dictionary of the activated neurons and save A[0] by the input of the network.\n",
    "    A_cache = {\"A0\": X}\n",
    "    Z_cache = {}\n",
    "    \n",
    "    for l in range(1, number_layers):\n",
    "        #Because the activation names start from the layer 1 I have to call as activation_name of layer 1 =\n",
    "        # activation_names[0], activation_names of layer 2 = activation_names[1] ...\n",
    "        activation_name = activation_names[l-1]\n",
    "        \n",
    "        #The weights and  biases defined by the dictionaries \n",
    "        A_prev = A_cache[\"A\" + str(l-1)]\n",
    "        W = weights[\"W\" + str(l)]\n",
    "        b = biases[\"b\" + str(l)]\n",
    "        \n",
    "        #Doing the linear activation and the forward activation in the layer l. It's worth to notice that I need\n",
    "        # to get Z because I will need those values for the backpropagation algorithm.\n",
    "        Z = linear_activation(W, A_prev, b)\n",
    "        A = forward_activation(W, A_prev, b, activation_name)\n",
    "        # Now I can save those values in the caches of A and Z\n",
    "        A_cache[\"A\" + str(l)] = A\n",
    "        Z_cache[\"Z\" +str(l)] = Z\n",
    "        \n",
    "  # Finally, I save the final value of the network Y, which is the value of the activation function in the last layer.    \n",
    "        \n",
    "    Y_hat = A_cache[\"A\" + str(number_layers-1)]\n",
    "        \n",
    "    return Z_cache, A_cache, Y_hat    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once the values have gone throught the network forward, I calculate the loss function.\n",
    "def loss_function(Y_hat, Y):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.12403434 0.06402207 0.09215076 0.090369   0.08300315 0.04543522\n",
      "  0.12778306 0.19087624 0.04394205 0.01967002 0.10814508 0.02636515\n",
      "  0.10655988 0.19087624 0.18228273 0.19087624 0.03197945 0.0495347\n",
      "  0.19087624 0.05101078 0.13398114 0.02175692 0.19087624 0.11149828\n",
      "  0.1015737  0.08718785 0.05587465 0.19087624 0.18749135 0.19087624\n",
      "  0.10216537 0.02141673 0.02928278 0.19087624 0.18142697 0.19087624\n",
      "  0.13694144 0.18121682 0.06332262 0.05358993 0.15310065 0.09712575\n",
      "  0.09311923 0.03316613 0.13189012 0.12344581 0.14175439 0.12866029\n",
      "  0.06295504 0.1870137  0.17814777 0.16224964 0.17552194 0.12073398\n",
      "  0.10811648 0.08923586 0.16234764 0.11000279 0.07715818 0.18635714\n",
      "  0.10532665 0.10126112 0.19087624 0.19016666 0.06813422 0.07761095\n",
      "  0.11957585 0.1512983  0.08063715 0.19087624 0.12993331 0.0801318\n",
      "  0.15749406 0.06740483 0.1883632  0.19087624 0.07716814 0.14526629\n",
      "  0.09261386 0.18404807 0.06853935 0.09381071 0.12132315 0.19087624\n",
      "  0.0594309  0.18905691 0.16492489 0.15180244 0.0745631  0.08560624\n",
      "  0.17361164 0.19087624 0.07946208 0.09600466 0.03476022 0.06950333\n",
      "  0.10209101 0.09267013 0.14389649 0.1113755 ]]\n"
     ]
    }
   ],
   "source": [
    "#Draft where I implement ideas\n",
    "X = np.random.randn(2,100)\n",
    "neurons_by_layer = [2, 3, 4, 2, 1]\n",
    "number_layers = len(neurons_by_layer)\n",
    "activation_names = ['relu', 'relu', 'relu', 'sigmoid']\n",
    "\n",
    "weights, biases = init_weights(number_layers, neurons_by_layer)\n",
    "\n",
    "Z_cache, A_cache, Y = forward_iteration(X, weights, biases, activation_names, number_layers, neurons_by_layer)\n",
    "\n",
    "Y = A_cache[\"A\" + str(number_layers-1)]\n",
    "\n",
    "print(Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
